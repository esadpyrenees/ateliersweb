# Scrappy

Le scrapping consiste Ã  effectuer des requÃªtes vers des services web (des sites, des API) pour en recevoir des informations et les enregistrer localement. Dans le cas dâ€™API, câ€™est souvent prÃ©vu et autorisÃ©. Dans le cas de sites web, câ€™est moins souvent autorisÃ©, et donc dâ€™autant plus utile :)

Cet exemple se propose de tÃ©lÃ©charger un ensemble dâ€™images [lÃ©galement mises Ã  disposition](https://www.loc.gov/free-to-use/) par la _Library of Congress_Â : un [ensemble de portraits](https://www.loc.gov/free-to-use/c-m-bell-studio-collection/) par Charles Milton Bell.

Cette opÃ©ration demandera dâ€™inspecter avec soin le code HTML des pages qui contiennent les ressources que lâ€™on veut tÃ©lÃ©charger. 

## Environnement virtuel

Python se manipule via la ligne de commande (rappel de quelques notions de [CLI â€“ _Command Line Interface_](../../cli/)).

Sous Windows, il est recommandÃ© dâ€™utiliser PowerShell[^wcli] plutÃ´t que lâ€™interprÃ©teur de commandes CMD. Au long de cette introduction, des dÃ©tails sont donnÃ©s pour cet environnement quand il diffÃ¨re de lâ€™environnement MacOs / Linux.

[^wcli]: Ou encore mieux, WSL. [Voir ici](../cli/).

Il est souhaitable dâ€™utiliser un _environnement virtuel_ pour installer des paquets Python dans un environnement isolÃ©, afin de stabiliser les versions des paquets utilisÃ©es dans un projet et Ã©viter les conflits.

On crÃ©e lâ€™environnement virtuel "env" en ligne de commandeÂ :

```sh
cd ~/mes_sites_web # pour se positionner dans lâ€™arborescence du site 
mkdir scrappy # crÃ©er un dossier pour lâ€™ensemble du projet
cd scrappy # rentrer dans ce nouveau dossier
python -m venv env # crÃ©er lâ€™environnement dans ce dossier
```

Cela va crÃ©er dans votre projet (ou ailleurs, selon lâ€™endroit oÃ¹ on lance la commande) un dossier `env` qui contiendra une copie de lâ€™exÃ©cutable Python et les paquets qui seront installÃ©s dans le projet.

Une fois lâ€™environnement virtuel crÃ©Ã©, il faut _lâ€™activer_Â :

```sh
source env/bin/activate
# Ou bien
. env/bin/activate
# NB: pour le dÃ©sactiver, on pourra faire :
# deactivate
```

<details> 
<summary>âš  Windows</summary>
<p>Il faut autoriser lâ€™Ã©xÃ©cution des scripts. Dans PowerShell, saisir : </p>
<pre><code class="language-sh language-bash">Set-ExecutionPolicy -ExecutionPolicy Bypass -Scope Process -Force</code>
</pre>
<p>Puis, pour activer lâ€™environnement virtuel :</p>
<pre><code class="language-sh language-bash">./env/Scripts/activate.ps1</code>
</pre>
</details>


### Installer les bibliothÃ¨ques utiles

On peut maintenant installer les bibliothÃ¨ques python qui seront nÃ©cessaires, grÃ¢ce Ã  `pip`, un outil dâ€™installation de paquets Python (notamment ceux prÃ©sents sur [PyPI](https://pypi.org/)). Elles seront installÃ©es avec leurs dÃ©pendances dans lâ€™environnement virtuel.

Les paquets ou bibliothÃ¨ques Python suivantes doivent Ãªtre installÃ©sÂ : `requests` (pour faire des requÃªtes HTTP), `BeautifulSoup` (puissante librairie de lecture / Ã©criture de HTML), `slugify` (ppour transformer des chaÃ®nes de caractÃ¨res en â€œslugsâ€, sans accents, espaces, caractÃ¨res spÃ©ciaux) et `lxml`, un simple â€œparserâ€ de XML (pour analyser le HTML).

```sh
pip install requests beautifulsoup4 python-slugify lxml
```


<details> 
<summary>âš  Windows</summary>
<p>Si la commande `pip` ne fonctionne pas, utiliser Ã  la place `python -m pip`.</p></details>

## Structure du projet

Une fois les bibliothÃ¨ques installÃ©es, on peut commencer Ã  Ã©crire notre script de scrapping, `scrap.py`, en le crÃ©ant dans le dossier grÃ¢ce Ã  notre Ã©diteur de texte favori.

```sh
mkdocs new mon_projet
```
Le projet se structure ainsi :

<pre markdown="0">
<span class="icon-folder-open"></span> scrappy
    <span class="icon-folder-open"></span> env
    <span class="icon-folder-open"></span> output
    <span class="icon-file-empty"></span> scrap.py
</pre>

- Le dossier `env` est celui de lâ€™environnement virtuelÂ ; on nâ€™y touche pas.
- Le sous-dosssier `output` servira Ã  stocker les rÃ©sultats de notre exploration. 
- Le fichier `scrap.py` est notre script principal.

## Scraping

Pour une meilleure comprÃ©hension du processus, lâ€™ensemble des opÃ©rations Ã  effectuer sera dÃ©coupÃ© en plusieurs Ã©tapes. PremiÃ¨re Ã©tape : la rÃ©colte des URLs des page de dÃ©tail des images.

### URLs des images

##### scrap.py (pseudo-code) {.filename}
```python
# On effectue une requÃªte vers "https://www.loc.gov/free-to-use/c-m-bell-studio-collection/"
# On lit le document HTML obtenu avec BeautifulSoup 
# On inspecte tous les Ã©lÃ©ments <figure> contenus dans le document pour rÃ©cupÃ©rer les URLs des pages de dÃ©tail de chaque image
# On stocke chaque URL dans une liste "urls"
# Pour chaque Ã©lÃ©ment de la liste, on effectue une nouvelle requÃªte
# On lit ce nouveau document avec BeautifulSoup 
# On rÃ©cupÃ¨re lâ€™URL de lâ€™image grÃ¢ce Ã  la balise <meta property="og:image">
# On stocke cette URL dans une liste "images"
# On enregistre cette liste en tant que fichier json
```
On peut lâ€™invoquer avecÂ :
```sh
python scrap.py
```

##### scrap.py {.filename}
```python
#!/usr/bin/env python3

# â†‘ cette premiÃ¨re ligne indique que la version de python Ã  utiliser 
# pour ce script est celle contenue dans lâ€™environnement virtuel

import requests
from bs4 import BeautifulSoup
import json

baseurl = "https://www.loc.gov"
loc = baseurl + "/free-to-use/c-m-bell-studio-collection/"

r = requests.get(loc)
soup = BeautifulSoup(r.text, "lxml")

urls = []
photos = soup.find_all("figure")
for p in photos:
  a = baseurl + p.find("a")["href"]
  urls.append(a)

images = []
for url in urls:
  r = requests.get(url)
  soup = BeautifulSoup(r.text, "lxml")
  metas = soup.find_all(attrs = {"property": "og:image"})
  images.append(metas[0]['content'])

with open('images.json', 'w') as f:
  json.dump(images, f)
```
Ã€ la suite de lâ€™exÃ©cution de ce script, un fichier `images.json` contient les urls de lâ€™ensemble des images Ã  tÃ©lÃ©chargerÂ :

<pre markdown="0">
<span class="icon-folder-open"></span> scrappy
    <span class="icon-folder-open"></span> env
    <span class="icon-folder-open"></span> output
    <span class="icon-file-empty"></span> images.json
    <span class="icon-file-empty"></span> scrap.py
</pre>

### TÃ©lÃ©chargement des images

Un nouveau script, `download.py`, va permettre de tÃ©lÃ©charger chacune des images dans le dossier `output`:

##### scrap.py (pseudo-code) {.filename}
```python
# On lit le fichier "images.json" qui contient les URLs des images
# On crÃ©e le dossier "output" sâ€™il nâ€™existe pas
# Pour chaque Ã©lÃ©ment de la liste dâ€™images, on dÃ©termine son chemin de destination 
# Si le chemin existe (lâ€™image a dÃ©jÃ  Ã©tÃ© tÃ©lÃ©chargÃ©e), on passe Ã  la suivante
# Sinon, on effectue une requÃªte vers lâ€™URL
# Si la requÃªte est fructueuse (status_code=200), on enregistre lâ€™image Ã  son emplacement de destination
```
On peut lâ€™invoquer avecÂ :
```sh
python download.py
```

##### download.py {.filename}
```python
#!/usr/bin/env python3

import requests
import json
import os

# load json
with open('images.json', 'r') as f:
  images = json.load(f)

# create output directory
base_dir = os.getcwd()
out_dir = os.path.join(base_dir, "output")
if not os.path.exists(out_dir):
  os.mkdir(out_dir)
print("Putting downloaded files into: " + out_dir)

# loop through images URLs list
for url in images:
  
  print(f"(Trying to download: {url}" )

  out_file = os.path.join(out_dir, os.path.basename(url))
  if os.path.isfile(out_file):
    print("Skipping, file already downloaded\n")
    continue
  
  try:
    r = requests.get(url)
  except:
    print("Request failed, skipping")
    continue

  if r.status_code == 200:
    with open(out_file, 'wb') as f:
      print(f"Writing image to: {out_file}")
      f.write(r.content)
  else:
    print("Error downloading, HTTP response code was: " + str(r.status_code))
    continue
```

Si tout sâ€™est bien passÃ© ğŸ¤, les images ont Ã©tÃ© tÃ©lÃ©chargÃ©es dans le dossier `output`.

## Traitement des images

La prochaine Ã©tape se proposera dâ€™utiliser le paquet python `face_recognition` pour dÃ©tecter automatiquement les visages Ã  lâ€™intÃ©rieur de ces portraits afin de produire des faux John Baldessari.

![John Baldessari](john-baldessari.jpg)

En attendant, on peut [Ã©couter Tom Waits](https://youtu.be/eU7V4GyEuXA?si=m558CoqP_HBnCMTN) raconter son histoire en six minutes.